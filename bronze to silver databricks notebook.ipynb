{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7c49f6-5bb1-48ea-9506-355b7ef83c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver Layer\n",
    "In this notebook, we will see how to do transformation and move the bronze data into silver folder after transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289e500d-40b7-4100-9a5a-4a1cbd995841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Connecting Databricks with ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f528e048-44dd-437e-a3ed-38648efb109d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define values\n",
    "storage_account = \"adlsfornyctaxi\"\n",
    "storage_container = \"bronze\"\n",
    "application_id = 'paste your application id here'\n",
    "secret = 'paste your secret here'\n",
    "tenant_id = 'paste your tenant id here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac6d89e-7a1d-4c3f-a358-3b0de316d6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Configure Spark\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1b3e2ba-3ea0-4938-90e3-37b45c54789f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Attempt to list directory contents\n",
    "path = f\"abfss://{storage_container}@{storage_account}.dfs.core.windows.net/2023/trip-data/\"\n",
    "dbutils.fs.ls(f\"abfss://{storage_container}@{storage_account}.dfs.core.windows.net/2023/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1e18c9-2732-41fa-9a58-7b68105d718e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Importing Libraies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d14493a-d13e-481f-91fe-a9da81501c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49d1286-73f9-4083-91dd-0013685ac495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How to read all files in a directory\n",
    "files = dbutils.fs.ls(path)\n",
    "for i in files:\n",
    "    print(i.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e272d1-1532-44e5-9604-2aa5ca3f1333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's First see the how the data is and let's decide the Column data type for one month and then we will assign the same data type to all other months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "358d559f-0c72-4396-b7e1-4b4fc4d9b9bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_jan = spark.read.parquet(\"abfss://bronze@adlsfornyctaxi.dfs.core.windows.net/2023/trip-data/green_tripdata_2023-01.parquet\")\n",
    "df_jan.describe().display()\n",
    "display(df_jan)\n",
    "df_jan.select(\"*\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a578442-f186-4d03-bd17-1d7e65fa6419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you observe, the jan file describes, row's, Column names and it's data types. Based on this we can understand the max value and min value as well. Based on this we need to create our own schema for the best practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49eec96-6416-4156-906d-547e61d62054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54fc63b2-6eab-4053-a6a3-66a7a45a1bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading the Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1732af4d-1f97-487b-9064-870b6954d3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_container = \"bronze\"\n",
    "files = dbutils.fs.ls(path)\n",
    "\n",
    "df_main = None\n",
    "\n",
    "for file in files:\n",
    "    df_temp = spark.read.format('parquet')\\\n",
    "                .option('header', True)\\\n",
    "                        .load(file.path)\n",
    "    if df_main is None:\n",
    "        df_main = df_temp\n",
    "    else:\n",
    "        df_main = df_main.union(df_temp)\n",
    "\n",
    "df_main.display()\n",
    "\n",
    "# Just to check whether it's stored all the files row data or not\n",
    "Total_rows_from_all_files = df_main.select(\"VendorID\").count()\n",
    "print(f\"Total rows from all files: {Total_rows_from_all_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92404cac-266d-4510-81c1-abbf3704ad63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Apply Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc0bce5-3ed0-4bee-934a-ebe1327384b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Converting the data types, if we don't do it, it will throw an error as datatype mismatch, to encounter this error, we need to convert the data types.\n",
    "df_main = df_main.withColumn(\"VendorID\", col(\"VendorID\").cast(LongType())) \\\n",
    "                 .withColumn(\"lpep_pickup_datetime\", col(\"lpep_pickup_datetime\").cast(TimestampNTZType())) \\\n",
    "                 .withColumn(\"lpep_dropoff_datetime\", col(\"lpep_dropoff_datetime\").cast(TimestampNTZType())) \\\n",
    "                 .withColumn(\"store_and_fwd_flag\", col(\"store_and_fwd_flag\").cast(StringType())) \\\n",
    "                 .withColumn(\"RatecodeID\", col(\"RatecodeID\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"PULocationID\", col(\"PULocationID\").cast(LongType())) \\\n",
    "                 .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(LongType())) \\\n",
    "                 .withColumn(\"passenger_count\", col(\"passenger_count\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"trip_distance\", col(\"trip_distance\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"fare_amount\", col(\"fare_amount\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"extra\", col(\"extra\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"mta_tax\", col(\"mta_tax\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"tip_amount\", col(\"tip_amount\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"tolls_amount\", col(\"tolls_amount\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"ehail_fee\", col(\"ehail_fee\").cast(IntegerType())) \\\n",
    "                 .withColumn(\"improvement_surcharge\", col(\"improvement_surcharge\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"total_amount\", col(\"total_amount\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"payment_type\", col(\"payment_type\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"trip_type\", col(\"trip_type\").cast(DoubleType())) \\\n",
    "                 .withColumn(\"congestion_surcharge\", col(\"congestion_surcharge\").cast(DoubleType()))\n",
    "\n",
    "\n",
    "# Dropping the ehail_fee column since it is filled with null\n",
    "df_main = df_main.drop(\"ehail_fee\")\n",
    "\n",
    "# Adding new column to find the time difference between pickup and dropoff\n",
    "df_main = df_main.withColumn(\"time_diff_pick_drop_min\", round(floor(unix_timestamp(col(\"lpep_dropoff_datetime\")) - unix_timestamp(col(\"lpep_pickup_datetime\"))) / 60))\n",
    "\n",
    "# Separating the date from the pickup time\n",
    "df_main = df_main.withColumn(\"trip_date\", to_date(col(\"lpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"trip_year\", year(col(\"lpep_pickup_datetime\")))\n",
    "\n",
    "# Selecting the required columns\n",
    "df_main = df_main.select(\"VendorID\", \"PULocationID\", \"DOLocationID\", \"passenger_count\", \"trip_distance\", \"fare_amount\", \"total_amount\", \"payment_type\", \"trip_type\", \"time_diff_pick_drop_min\", \"trip_date\", \"trip_year\", \"lpep_pickup_datetime\", \"lpep_dropoff_datetime\")\n",
    "\n",
    "display(df_main)\n",
    "df_main.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a343279f-2a3a-4185-82a4-69d81ed4f395",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Writing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff78eff-96ad-46f5-be0f-02d1501b98f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Write the main DataFrame to the silver container\n",
    "output_path = f\"abfss://silver@{storage_account}.dfs.core.windows.net/2023/\"\n",
    "df_main.write.format(\"parquet\").mode(\"append\").option(\"path\", output_path).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e9a1c45-dfb0-43e6-a890-e24fbef02437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> The Next task is to prepare the data for the Gold Layer, check the Gold Layer Databricks notebook."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze to silver databricks notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
